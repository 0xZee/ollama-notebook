{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNK9YuBVYCULiO/1GDUKA6V"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain-community"
      ],
      "metadata": {
        "id": "Y1MQ3_dKew3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9N1tK3AbXErK",
        "outputId": "9fcafa32-2ad6-43cc-f51a-b524956fd230"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Downloading ollama...\n",
            "############################################################################################# 100.0%\n",
            ">>> Installing ollama to /usr/local/bin...\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "#Download ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get ollama serve running on a different thread\n",
        "\n",
        "import subprocess\n",
        "process = subprocess.Popen(\"ollama serve\", shell=True) #runs on a different thread\n"
      ],
      "metadata": {
        "id": "iUsho5QzXLHD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download model (llama3)\n",
        "!ollama pull llama3\n",
        "!pip install ollama\n"
      ],
      "metadata": {
        "id": "qdEGm1CfXNMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "\n",
        "# Generate OLLAMA RESPONSE\n",
        "def gen_ollama(PROMPT):\n",
        "  response = ollama.chat(model='llama3', messages=[\n",
        "    {\n",
        "      'role': 'user',\n",
        "      'content': PROMPT,\n",
        "    },\n",
        "  ])\n",
        "  return response['message']['content']\n",
        "\n",
        "# Get Ollama Answer\n",
        "PROMPT = \"Can you list and explain the types of ML in bullets format \"\n",
        "gen_ollama(PROMPT)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "collapsed": true,
        "id": "0Fr3qP8MXPDP",
        "outputId": "5a8cbe2b-c941-4522-cf1b-977b88c86c76"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Here are the main types of Machine Learning (ML) in bullet points:\\n\\n• **Supervised Learning**:\\n\\t+ Involves training a model on labeled data, where each example is accompanied by its correct output or label.\\n\\t+ The goal is to learn a mapping between inputs and outputs that can be used to make predictions on new, unseen data.\\n\\t+ Examples: Classification (e.g., spam vs. non-spam emails), Regression (e.g., predicting house prices).\\n\\n• **Unsupervised Learning**:\\n\\t+ Involves training a model on unlabeled data, where the goal is to discover hidden patterns or structure in the data.\\n\\t+ No labeled examples are available for training; instead, the model must find meaningful relationships within the data itself.\\n\\t+ Examples: Clustering (e.g., grouping similar customers), Dimensionality Reduction (e.g., reducing high-dimensional data to a lower dimension).\\n\\n• **Semi-supervised Learning**:\\n\\t+ Combines supervised and unsupervised learning by using both labeled and unlabeled data for training.\\n\\t+ The model learns from the labeled examples while also exploiting any structure present in the unlabeled data.\\n\\t+ Examples: Training a classification model on a small labeled dataset, then fine-tuning it on a larger unlabeled dataset.\\n\\n• **Reinforcement Learning**:\\n\\t+ Involves training an agent to make decisions in an environment where its actions affect the state of the environment.\\n\\t+ The goal is to maximize a reward signal or minimize a penalty signal, often through trial and error.\\n\\t+ Examples: Training a robot arm to pick up objects, playing games like Go or Poker.\\n\\n• **Deep Learning**:\\n\\t+ A subset of machine learning that uses neural networks with multiple layers to learn complex patterns in data.\\n\\t+ Often used for image, speech, and natural language processing tasks.\\n\\t+ Examples: Image recognition (e.g., recognizing faces), Natural Language Processing (NLP) applications like language translation or text summarization.\\n\\n• **Transfer Learning**:\\n\\t+ Involves using a pre-trained model as the starting point for training on a new task or dataset.\\n\\t+ The pre-trained model has already learned general features or patterns that can be adapted to the new task.\\n\\t+ Examples: Using a pre-trained convolutional neural network (CNN) for image classification, then fine-tuning it for a specific object detection task.\\n\\n• **Online Learning**:\\n\\t+ Involves training a model on data streams or batches, where each example is processed one at a time.\\n\\t+ The goal is to adapt the model in real-time as new data becomes available, often without human intervention.\\n\\t+ Examples: Training a recommendation system to suggest products based on user behavior, adapting a spam filter to detect new types of spam emails.\\n\\n• **Batch Learning**:\\n\\t+ Involves training a model on batches of data, where each batch is processed separately before moving on to the next one.\\n\\t+ The goal is to optimize the model's performance on the entire dataset by processing it in chunks.\\n\\t+ Examples: Training a classification model on a large dataset, then testing its performance on a separate validation set.\\n\\n• **Meta-Learning**:\\n\\t+ Involves training a model to learn from other models or from itself, often through self-play or multi-agent learning.\\n\\t+ The goal is to develop models that can quickly adapt to new tasks or environments by leveraging knowledge gained from previous experiences.\\n\\t+ Examples: Training a model to learn from its own mistakes and improve over time, developing a meta-learning algorithm for few-shot learning.\\n\\nThese categories are not mutually exclusive, as many ML applications combine multiple types.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OLLAMA STREAMING\n",
        "def gen_stream_ollama(PROMPT):\n",
        "\n",
        "  stream = ollama.chat(\n",
        "    model='llama3',\n",
        "    messages=[{'role': 'user', 'content': PROMPT}],\n",
        "    stream=True\n",
        "  )\n",
        "\n",
        "  for chunk in stream:\n",
        "    return chunk['message']['content'], end='', flush=True"
      ],
      "metadata": {
        "id": "euS77bl9fVm8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_stream_ollama(PROMPT=\"Give me simple code for langchain\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAqiRWG8em0X",
        "outputId": "d7279b41-1de5-4b84-dd36-1be722a3bdd2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A language model!\n",
            "\n",
            "Here's a simple implementation of LangChain, a neural network-based language generator:\n",
            "```python\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "import torch.optim as optim\n",
            "from torch.utils.data import Dataset, DataLoader\n",
            "\n",
            "class LangDataset(Dataset):\n",
            "    def __init__(self, text_data, seq_len=128):\n",
            "        self.text_data = text_data\n",
            "        self.seq_len = seq_len\n",
            "\n",
            "    def __len__(self):\n",
            "        return len(self.text_data)\n",
            "\n",
            "    def __getitem__(self, idx):\n",
            "        text = self.text_data[idx]\n",
            "        input_seq = [self.text_data[idx][i] for i in range(min(idx+1, self.seq_len))]\n",
            "        target_seq = [self.text_data[idx][i] for i in range(min(idx+seq_len, len(self.text_data)))]\n",
            "\n",
            "        return {\n",
            "            'input': torch.tensor(input_seq),\n",
            "            'target': torch.tensor(target_seq)\n",
            "        }\n",
            "\n",
            "class LangModel(nn.Module):\n",
            "    def __init__(self, vocab_size, emb_dim=128, hidden_dim=256, num_layers=2):\n",
            "        super(LangModel, self).__init__()\n",
            "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
            "        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers)\n",
            "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
            "\n",
            "    def forward(self, x):\n",
            "        x = self.emb(x)\n",
            "        x = torch.relu(self.lstm(x)[0])\n",
            "        return self.fc(x)\n",
            "\n",
            "def train(model, device, train_data, batch_size=32):\n",
            "    model.train()\n",
            "    criterion = nn.CrossEntropyLoss()\n",
            "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
            "\n",
            "    for batch in DataLoader(train_data, batch_size=batch_size):\n",
            "        input_seq, target_seq = batch['input'], batch['target']\n",
            "        input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
            "        optimizer.zero_grad()\n",
            "\n",
            "        output = model(input_seq)\n",
            "        loss = criterion(output, target_seq)\n",
            "        loss.backward()\n",
            "        optimizer.step()\n",
            "\n",
            "    return loss.item()\n",
            "\n",
            "def generate_text(model, device, prompt, max_length=50):\n",
            "    model.eval()\n",
            "    input_seq = torch.tensor([word2idx[prompt[0]]])\n",
            "    for i in range(1, len(prompt)):\n",
            "        input_seq = torch.cat((input_seq, torch.tensor([word2idx[prompt[i]]])))\n",
            "\n",
            "    output = []\n",
            "    while len(output) < max_length:\n",
            "        output.append(input_seq[-1].item())\n",
            "        input_seq = input_seq.view(-1, 1).to(device)\n",
            "        with torch.no_grad():\n",
            "            next_word_idx = model(input_seq).argmax(dim=1)\n",
            "        input_seq = torch.cat((input_seq[1:], torch.tensor([next_word_idx])))\n",
            "\n",
            "    return ''.join([idx2word[i] for i in output])\n",
            "\n",
            "# Load your dataset and vocabulary\n",
            "text_data = ...\n",
            "vocab_size = ...\n",
            "\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "model = LangModel(vocab_size=vocab_size)\n",
            "train(model, device, LangDataset(text_data), batch_size=32)\n",
            "\n",
            "prompt = \"Your prompt here...\"\n",
            "generated_text = generate_text(model, device, prompt)\n",
            "print(generated_text)\n",
            "```\n",
            "This code is a basic implementation of LangChain. It defines:\n",
            "\n",
            "1. A `LangDataset` class to create batches from the input text data.\n",
            "2. A `LangModel` class that uses an LSTM network with an embedding layer and a fully connected output layer.\n",
            "3. A `train` function to train the model using the Adam optimizer and cross-entropy loss.\n",
            "4. A `generate_text` function to generate text based on a prompt.\n",
            "\n",
            "Note that this is a simplified example, and you may want to add more features (e.g., attention mechanisms) or fine-tune hyperparameters for better performance. Additionally, you'll need to preprocess your dataset and create a vocabulary mapping words to indices (`word2idx`) and vice versa (`idx2word`)."
          ]
        }
      ]
    }
  ]
}